"""
Evaluation pipeline for the Forex Multi-Agent system.

Analyzes trace logs generated by graph.py to measure:
‚úÖ Success/failure rates
‚úÖ Average confidence per pair
‚úÖ Action distribution (BUY/SELL/AVOID)
‚úÖ Step timing performance (avg/min/max)
‚úÖ Common failure causes
‚úÖ Category-level metrics (Major / Minor / Exotic)
‚úÖ Invalid/skipped pair tracking

Usage:
    python -m src.evaluation.eval_pipeline
"""

import os
import json
import statistics
from datetime import datetime, timezone
from collections import defaultdict

# Location of trace logs
TRACE_DIR = os.path.join(os.path.dirname(__file__), "..", "data", "traces")
EXPORT_PATH = os.path.join(os.path.dirname(__file__), "..", "data", "evaluation_summary.json")


# ----------------------------
# Helpers
# ----------------------------
def load_traces():
    """Load all saved JSON trace files from data/traces directory."""
    traces = []
    if not os.path.exists(TRACE_DIR):
        print("‚ö†Ô∏è No trace directory found:", TRACE_DIR)
        return traces

    for file in os.listdir(TRACE_DIR):
        if not file.endswith(".json"):
            continue
        path = os.path.join(TRACE_DIR, file)
        try:
            with open(path, "r", encoding="utf-8") as f:
                traces.append(json.load(f))
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to load {file}: {e}")
    return traces


def _to_dt(ts):
    """Convert timestamp to a timezone-aware datetime (UTC)."""
    if not ts:
        return None
    try:
        if isinstance(ts, str):
            dt = datetime.fromisoformat(ts.replace("Z", "+00:00"))
        else:
            dt = ts
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except Exception:
        return None


def compute_step_durations(trace):
    """Compute durations (seconds) between consecutive steps in a trace."""
    durations = {}
    steps = trace.get("steps", []) or []
    for i in range(len(steps) - 1):
        start = _to_dt(steps[i].get("ts"))
        end = _to_dt(steps[i + 1].get("ts"))
        if start and end:
            try:
                durations[steps[i].get("step", f"step_{i}")] = (end - start).total_seconds()
            except Exception:
                continue
    return durations


# ----------------------------
# Main Evaluation Logic
# ----------------------------
def summarize_traces(traces):
    """
    Aggregate metrics across all traces.
    Includes per-pair and per-category summaries.
    """
    summary = defaultdict(lambda: {
        "count": 0,
        "success": 0,
        "skipped_invalid": 0,
        "avg_conf": [],
        "stances": defaultdict(int),
        "failures": defaultdict(int),
        "timings": defaultdict(list),
        "category": "unknown",
    })

    for t in traces:
        pair = t.get("pair", "UNKNOWN")
        category = t.get("pair_category", "unknown")
        valid = t.get("pair_valid", True)
        status = t.get("status", "unknown")

        summary[pair]["category"] = category
        summary[pair]["count"] += 1

        # Track invalid/skipped
        if not valid or status == "skipped_invalid":
            summary[pair]["skipped_invalid"] += 1
            continue

        # Success/failure stats
        if status == "success":
            summary[pair]["success"] += 1
        else:
            err_type = (t.get("error") or "UnknownError").split(":")[0].strip()[:120]
            summary[pair]["failures"][err_type] += 1

        # Extract confidence + stance from steps
        for step in t.get("steps", []):
            if step.get("step") == "strategy_tool_end":
                conf = step.get("confidence")
                stance = step.get("stance", "UNKNOWN")
                if conf is not None:
                    try:
                        summary[pair]["avg_conf"].append(float(conf))
                    except Exception:
                        pass
                summary[pair]["stances"][stance] += 1

        # Timing breakdown
        durations = compute_step_durations(t)
        for step_name, sec in durations.items():
            try:
                summary[pair]["timings"][step_name].append(float(sec))
            except Exception:
                continue

    # Compute final per-pair stats
    final_pairs = {}
    for pair, data in summary.items():
        avg_conf = round(statistics.mean(data["avg_conf"]), 2) if data["avg_conf"] else 0.0
        success_rate = (
            round((data["success"] / max(data["count"] - data["skipped_invalid"], 1)) * 100, 1)
            if data["count"] > 0
            else 0.0
        )

        step_times = {}
        for step_key, values in data["timings"].items():
            if values:
                step_times[step_key] = {
                    "avg": round(statistics.mean(values), 2),
                    "min": round(min(values), 2),
                    "max": round(max(values), 2),
                    "samples": len(values),
                }

        final_pairs[pair] = {
            "category": data["category"],
            "runs": data["count"],
            "skipped_invalid": data["skipped_invalid"],
            "success_rate": f"{success_rate}%",
            "avg_confidence": avg_conf,
            "stances": dict(data["stances"]),
            "failures": dict(data["failures"]),
            "timings": step_times,
        }

    # Compute category-level aggregation
    categories = defaultdict(lambda: {"pairs": 0, "runs": 0, "success": 0, "avg_conf": [], "failures": 0})
    for pair, stats in final_pairs.items():
        cat = stats["category"]
        categories[cat]["pairs"] += 1
        categories[cat]["runs"] += stats["runs"]
        categories[cat]["success"] += int(float(stats["success_rate"].replace("%", "")) > 0)
        categories[cat]["avg_conf"].append(stats["avg_confidence"])
        categories[cat]["failures"] += len(stats["failures"])

    final_cats = {}
    for cat, v in categories.items():
        avg_conf_cat = round(statistics.mean(v["avg_conf"]), 2) if v["avg_conf"] else 0.0
        success_rate_cat = round((v["success"] / max(v["pairs"], 1)) * 100, 1)
        final_cats[cat] = {
            "pairs": v["pairs"],
            "runs": v["runs"],
            "avg_confidence": avg_conf_cat,
            "success_rate": f"{success_rate_cat}%",
            "failure_types": v["failures"],
        }

    return final_pairs, final_cats


# ----------------------------
# Runner / CLI
# ----------------------------
def run_evaluation():
    traces = load_traces()
    if not traces:
        print("‚ö†Ô∏è No traces found to evaluate.")
        return

    print(f"\nüìä Loaded {len(traces)} trace files for evaluation.")
    pair_summary, cat_summary = summarize_traces(traces)

    print("\n=== Forex Multi-Agent Evaluation Summary ===")
    for pair, stats in pair_summary.items():
        print(f"\nüîπ {pair} ({stats['category'].upper()})")
        print(f"  - Total Runs: {stats['runs']}")
        print(f"  - Success Rate: {stats['success_rate']}")
        print(f"  - Avg Confidence: {stats['avg_confidence']}")
        print(f"  - Stances: {stats['stances']}")
        if stats["skipped_invalid"]:
            print(f"  - ‚è≠Ô∏è Skipped Invalid: {stats['skipped_invalid']}")
        if stats["failures"]:
            print(f"  - ‚ö†Ô∏è Failures: {stats['failures']}")
        if stats["timings"]:
            print("  - ‚è± Step Timing (avg/min/max in sec):")
            for step_name, m in stats["timings"].items():
                print(f"      {step_name}: avg={m['avg']} min={m['min']} max={m['max']} (n={m['samples']})")

    print("\n=== Category-Level Summary ===")
    for cat, stats in cat_summary.items():
        print(f"\nüìà {cat.upper()} Pairs:")
        print(f"  - Pairs Count: {stats['pairs']}")
        print(f"  - Total Runs: {stats['runs']}")
        print(f"  - Success Rate: {stats['success_rate']}")
        print(f"  - Avg Confidence: {stats['avg_confidence']}")
        print(f"  - Distinct Failure Types: {stats['failure_types']}")

    # Export results to JSON for dashboards
    export_data = {
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "pairs": pair_summary,
        "categories": cat_summary,
    }
    with open(EXPORT_PATH, "w", encoding="utf-8") as f:
        json.dump(export_data, f, indent=2)

    print(f"\nüìÅ Results exported to: {EXPORT_PATH}")
    print("\n‚úÖ Evaluation complete.\n")


if __name__ == "__main__":
    run_evaluation()
